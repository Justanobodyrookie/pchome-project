import pandas as pd
import boto3
import json
import os
from sqlalchemy import create_engine
from sqlalchemy import inspect
from botocore.config import Config
from dotenv import load_dotenv
from datetime import datetime, timezone

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv('/home/hsu00093/.env')

# è¨­å®šè®Šæ•¸
DB_USER = 'user'
DB_PASSWORD = os.getenv('DB_PASSWORD')
DB_HOST = 'localhost'
DB_PORT = '5432'
DB_NAME = 'pchome_db'

MINIO_ENDPOINT = 'http://localhost:9000'
MINIO_ACCESS_KEY = 'admin'
MINIO_SECRET_KEY = os.getenv('MINIO_PASSWORD')

# ==========================================
# [Block 1] Extract: å¾ MinIO è®€å– (ä¿®æ­£ç‰ˆï¼šçªç ´ 1000 ç­†é™åˆ¶)
# ==========================================
def load_from_minio():
    print("Step 1 é–‹å§‹è®€å– MinIO...")
    try:
        s3 = boto3.client(
            's3',
            endpoint_url=MINIO_ENDPOINT,
            aws_access_key_id=MINIO_ACCESS_KEY,
            aws_secret_access_key=MINIO_SECRET_KEY,
            region_name='us-east-1',
            config=Config(connect_timeout=5, retries={'max_attempts': 0})
        )
        
        bucket_name = 'raw-data'
        
        # [é—œéµä¿®æ­£] æ”¹ç”¨ Paginatorï¼Œå°ˆé–€è™•ç†è¶…é 1000 å€‹æª”æ¡ˆçš„æƒ…æ³
        paginator = s3.get_paginator('list_objects_v2')
        page_iterator = paginator.paginate(Bucket=bucket_name, Prefix='pchome/')
        
        today = datetime.now(timezone.utc).date()
        target_files = []
        
        print("æ­£åœ¨æƒæ MinIO æª”æ¡ˆæ¸…å–® (å¯èƒ½éœ€è¦ä¸€é»æ™‚é–“)...")
        
        for page in page_iterator:
            if 'Contents' not in page:
                continue
                
            for obj in page['Contents']:
                # åªæŠ“ä»Šå¤©çš„
                if obj['LastModified'].date() == today:
                    target_files.append(obj['Key'])
        
        if not target_files:
            print(f"âš ï¸ æ‰¾ä¸åˆ°æ—¥æœŸç‚º {today} çš„æª”æ¡ˆ")
            return None

        print(f"ğŸ“¦ ç™¼ç¾ {len(target_files)} å€‹ä»Šå¤©çš„æª”æ¡ˆï¼Œé–‹å§‹ä¸‹è¼‰åˆä½µ...")
        
        data_list = []
        # ç‚ºäº†é¡¯ç¤ºé€²åº¦ï¼Œæ¯è™•ç† 100 å€‹å°ä¸€æ¬¡
        count = 0
        for key in target_files:
            try:
                obj = s3.get_object(Bucket=bucket_name, Key=key)
                content = json.loads(obj['Body'].read().decode('utf-8'))
                data_list.append(content)
                count += 1
                if count % 500 == 0:
                    print(f"  - å·²ä¸‹è¼‰ {count} / {len(target_files)} ç­†...")
            except Exception as e:
                print(f"è®€å– {key} å¤±æ•—: {e}")

        if not data_list:
            return None
            
        return pd.DataFrame(data_list)
        
    except Exception as e:
        print(f"MinIO è®€å–å¤±æ•—: {e}")
        return None

# ==========================================
# [Block 2] Transform: æ¸…æ´—èˆ‡æ‹†åˆ†
# ==========================================
def transform_data(df):
    print(f"Step 2 é–‹å§‹æ¸…æ´—è³‡æ–™ ({len(df)} ç­†)...")
    
    expected_dim_cols = ['product_id', 'name', 'category', 'describe', 'img_url']
    for col in expected_dim_cols:
        if col not in df.columns:
            df[col] = None
            
    expected_fact_cols = ['product_id', 'price', 'original_price', 'rating', 'comment', 'crawled_at']
    for col in expected_fact_cols:
        if col not in df.columns:
            df[col] = None

    # Dim è¡¨ï¼šå»é‡
    dim_df = df[expected_dim_cols].copy()
    dim_df.drop_duplicates(subset=['product_id'], keep='last', inplace=True)
    
    # Fact è¡¨
    fact_df = df[expected_fact_cols].copy()
    fact_df['rating'] = pd.to_numeric(fact_df['rating'], errors='coerce')
    fact_df['price'] = pd.to_numeric(fact_df['price'], errors='coerce')
    fact_df['original_price'] = pd.to_numeric(fact_df['original_price'], errors='coerce')
    fact_df['crawled_at'] = pd.to_datetime(fact_df['crawled_at'])
    
    return dim_df, fact_df

# ==========================================
# [Block 3] Load: å¯«å…¥ Postgres
# ==========================================
def load_to_postgres(dim_df, fact_df):
    print("Step 3 æº–å‚™å¯«å…¥è³‡æ–™åº«...")
    
    conn_str = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'
    engine = create_engine(conn_str)
    
    try:
        # 1. è™•ç† Dim Products
        print(f"æ­£åœ¨è™•ç† dim_products ({len(dim_df)} ç­†)...")
        inspector = inspect(engine)
        
        if not inspector.has_table("dim_products"):
            print("å»ºç«‹æ–°è¡¨ dim_products...")
            dim_df.to_sql('dim_products', engine, if_exists='append', index=False)
        else:
            existing_ids = pd.read_sql("SELECT product_id FROM dim_products", engine)
            # é€™è£¡è¦å°å¿ƒï¼Œå¦‚æœè³‡æ–™åº«æ˜¯ç©ºçš„ï¼Œexisting_ids å¯èƒ½æœƒå ±éŒ¯æˆ–ç©º
            if not existing_ids.empty:
                new_products = dim_df[~dim_df['product_id'].isin(existing_ids['product_id'])]
            else:
                new_products = dim_df

            if not new_products.empty:
                print(f"å¯«å…¥ {len(new_products)} ç­†æ–°å•†å“...")
                new_products.to_sql('dim_products', engine, if_exists='append', index=False, method='multi')
            else:
                print("æ²’æœ‰æ–°å•†å“éœ€è¦å¯«å…¥ã€‚")

        # 2. è™•ç† Fact Daily Prices
        print(f"å¯«å…¥ fact_daily_prices ({len(fact_df)} ç­†)...")
        # chunksize è¨­ç‚º 1000ï¼Œé¿å…ä¸€æ¬¡å¡å¤ªå¤šè¢«è³‡æ–™åº«è¸¢å‡ºä¾†
        fact_df.to_sql('fact_daily_prices', engine, if_exists='append', index=False, method='multi', chunksize=1000)
        
        print("ğŸ‰ å¤§åŠŸå‘Šæˆï¼")
        
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    df_raw = load_from_minio()
    if df_raw is not None and not df_raw.empty:
        dim, fact = transform_data(df_raw)
        load_to_postgres(dim, fact)
    else:
        print("æ²’æœ‰è³‡æ–™éœ€è¦è™•ç†ã€‚")
